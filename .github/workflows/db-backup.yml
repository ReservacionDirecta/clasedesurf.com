name: üíæ Database Backup

on:
  schedule:
    # Run daily at 05:00 UTC
    - cron: "0 5 * * *"
  workflow_dispatch:
    # Allow manual trigger

jobs:
  backup:
    name: üõ°Ô∏è Backup Database
    runs-on: ubuntu-latest

    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4

      - name: üöÇ Install Railway CLI
        run: npm install -g @railway/cli

      - name: üêò Create Backup
        env:
          RAILWAY_TOKEN: ${{ secrets.RAILWAY_TOKEN }}
          RAILWAY_PROJECT_ID: 5b9527ff-cf02-405e-989f-f1120848d679
          CI: true
        run: |
          # Create backups directory
          mkdir -p backups

          # Generate timestamp
          TIMESTAMP=$(date +%Y-%m-%d_%H-%M-%S)
          FILENAME="backup_${TIMESTAMP}.sql"
          FILEPATH="backups/${FILENAME}"

          echo "Generating backup: $FILENAME"

          # Install pg_dump
          sudo apt-get update && sudo apt-get install -y postgresql-client

          # Get Database URL
          # Use json output to parse cleaner if needed, but 'variables get' usually returns raw value
          DATABASE_URL=$(railway variables get DATABASE_URL --service surfschool-backend)

          if [ -z "$DATABASE_URL" ]; then
            echo "::error::Could not retreive DATABASE_URL"
            exit 1
          fi

          echo "Dumping database..."
          pg_dump "$DATABASE_URL" --no-owner --no-privileges --clean --if-exists --file="$FILEPATH"

          # Compress
          echo "Compressing..."
          gzip "$FILEPATH"
          echo "Backup created: ${FILEPATH}.gz"

          # NOTE: To use the /storage/data volume for backups, this script would need to run ON the Railway service,
          # not on GitHub Actions runner. GitHub Runners are ephemeral.
          # For offsite backups without AWS, consider other storage providers or Railway's own backup features.
          # For now, this workflow verifies the DB is dumpable and creates an artifact.

      - name: üì¶ Upload Artifact
        uses: actions/upload-artifact@v4
        with:
          name: database-backup
          path: backups/*.gz
          retention-days: 5

      # - name: ‚òÅÔ∏è Upload to S3/R2 (DISABLED - Users specified no AWS)
      #   if: success()
      #   uses: aws-actions/configure-aws-credentials@v4
      #   with:
      #     aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
      #     aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      #     aws-region: ${{ secrets.AWS_REGION }}
      #
      # - name: üì§ Copy to Bucket
      #   if: success()
      #   env:
      #     BUCKET: ${{ secrets.AWS_BUCKET_NAME }}
      #   run: |
      #     aws s3 cp backups/ s3://$BUCKET/backups/ --recursive --exclude "*" --include "*.gz"
